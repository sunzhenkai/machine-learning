{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "183d5212-dfa7-4d83-afd7-8fdb039d946f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T08:34:00.961681Z",
     "iopub.status.busy": "2025-06-22T08:34:00.960993Z",
     "iopub.status.idle": "2025-06-22T08:34:00.979487Z",
     "shell.execute_reply": "2025-06-22T08:34:00.977731Z",
     "shell.execute_reply.started": "2025-06-22T08:34:00.961610Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download from: https://criteostorage.blob.core.windows.net/criteo-research-datasets/kaggle-display-advertising-challenge-dataset.tar.gz\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1603df2a-b024-4f47-844c-1cc485b56be8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T08:34:01.970770Z",
     "iopub.status.busy": "2025-06-22T08:34:01.970223Z",
     "iopub.status.idle": "2025-06-22T08:34:01.977530Z",
     "shell.execute_reply": "2025-06-22T08:34:01.976098Z",
     "shell.execute_reply.started": "2025-06-22T08:34:01.970708Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = ('hdfs://namenode/dataset/demo/readme.txt', 'hdfs://namenode/dataset/demo/train.txt', 'hdfs://namenode/dataset/demo/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03caaa36-d1de-4164-8903-89e6771efa73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T08:34:02.971083Z",
     "iopub.status.busy": "2025-06-22T08:34:02.970368Z",
     "iopub.status.idle": "2025-06-22T08:34:07.583039Z",
     "shell.execute_reply": "2025-06-22T08:34:07.580879Z",
     "shell.execute_reply.started": "2025-06-22T08:34:02.971015Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/06/22 16:34:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.master(\"spark://datascience-spark:7077\")\\\n",
    "    .config(\"spark.pyspark.python\", \"/opt/bitnami/python/bin/python3\")\\\n",
    "    .config(\"spark.executor.memory\", \"4g\")\\\n",
    "    .config(\"spark.driver.memory\", \"4g\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cbc0729-356c-412c-8d90-bd8008836238",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T08:34:10.703020Z",
     "iopub.status.busy": "2025-06-22T08:34:10.702231Z",
     "iopub.status.idle": "2025-06-22T08:34:10.710158Z",
     "shell.execute_reply": "2025-06-22T08:34:10.708496Z",
     "shell.execute_reply.started": "2025-06-22T08:34:10.702939Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# conf = SparkConf()\n",
    "\n",
    "# conf.setAppName('metaspore-demo')\n",
    "# conf.setMaster('spark://datascience-spark:7077')\n",
    "# sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dce74f12-8c56-4f5f-814b-1d10142932aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T08:34:13.140658Z",
     "iopub.status.busy": "2025-06-22T08:34:13.139977Z",
     "iopub.status.idle": "2025-06-22T08:34:15.304648Z",
     "shell.execute_reply": "2025-06-22T08:34:15.303282Z",
     "shell.execute_reply.started": "2025-06-22T08:34:13.140590Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: hdfs://namenode/dataset/demo/readme.txt",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m readme  \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m readme\u001b[38;5;241m.\u001b[39mcollect():\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(line[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py:421\u001b[0m, in \u001b[0;36mDataFrameReader.text\u001b[0;34m(self, paths, wholetext, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter)\u001b[0m\n\u001b[1;32m    419\u001b[0m     paths \u001b[38;5;241m=\u001b[39m [paths]\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: hdfs://namenode/dataset/demo/readme.txt"
     ]
    }
   ],
   "source": [
    "readme  = spark.read.text(dataset[0])\n",
    "for line in readme.collect():\n",
    "    print(line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34c82f94-a79a-438e-a2d2-95b3a80e7811",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T07:57:15.742766Z",
     "iopub.status.busy": "2025-01-19T07:57:15.742089Z",
     "iopub.status.idle": "2025-01-19T07:57:18.260447Z",
     "shell.execute_reply": "2025-01-19T07:57:18.258580Z",
     "shell.execute_reply.started": "2025-01-19T07:57:15.742699Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_test = spark.read.csv(dataset[2], header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2e82e65-7231-419e-bcba-0d8a0f5b08b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T07:57:18.906560Z",
     "iopub.status.busy": "2025-01-19T07:57:18.905905Z",
     "iopub.status.idle": "2025-01-19T07:57:18.913132Z",
     "shell.execute_reply": "2025-01-19T07:57:18.911526Z",
     "shell.execute_reply.started": "2025-01-19T07:57:18.906495Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6dc0c4c-cff2-472d-99fd-237142a9ed08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T07:57:19.190539Z",
     "iopub.status.busy": "2025-01-19T07:57:19.189973Z",
     "iopub.status.idle": "2025-01-19T07:57:19.196732Z",
     "shell.execute_reply": "2025-01-19T07:57:19.195140Z",
     "shell.execute_reply.started": "2025-01-19T07:57:19.190477Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dd60d18-e408-423d-a0e6-9bf99edb9237",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T07:57:19.618362Z",
     "iopub.status.busy": "2025-01-19T07:57:19.617794Z",
     "iopub.status.idle": "2025-01-19T07:57:20.168525Z",
     "shell.execute_reply": "2025-01-19T07:57:20.166964Z",
     "shell.execute_reply.started": "2025-01-19T07:57:19.618301Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = spark.read.csv(dataset[1], header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e8aa9af-7b5f-47ff-8fdf-98e26ac87edd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T07:57:21.203016Z",
     "iopub.status.busy": "2025-01-19T07:57:21.202335Z",
     "iopub.status.idle": "2025-01-19T07:57:26.810796Z",
     "shell.execute_reply": "2025-01-19T07:57:26.809303Z",
     "shell.execute_reply.started": "2025-01-19T07:57:21.202949Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:====================================================>    (77 + 7) / 84]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45840617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#df_train.show()\n",
    "print(df_train.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3770fea5-3773-4400-991f-c307e9ed84ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T07:57:27.839372Z",
     "iopub.status.busy": "2025-01-19T07:57:27.838692Z",
     "iopub.status.idle": "2025-01-19T07:57:27.846464Z",
     "shell.execute_reply": "2025-01-19T07:57:27.844505Z",
     "shell.execute_reply.started": "2025-01-19T07:57:27.839307Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROOT_DIR = '/home/dev/workspace/machine-learning/playground/datascience/data'\n",
    "# ROOT_DIR = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a62c541e-79d9-4005-9412-4458cbd2ff22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T07:57:28.914015Z",
     "iopub.status.busy": "2025-01-19T07:57:28.913381Z",
     "iopub.status.idle": "2025-01-19T07:57:30.290483Z",
     "shell.execute_reply": "2025-01-19T07:57:30.288643Z",
     "shell.execute_reply.started": "2025-01-19T07:57:28.913953Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dev/workspace/machine-learning/playground/datascience\n"
     ]
    }
   ],
   "source": [
    "!mkdir data/data\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30866915-9296-4290-a992-1824cf008acd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T07:57:30.294126Z",
     "iopub.status.busy": "2025-01-19T07:57:30.293549Z",
     "iopub.status.idle": "2025-01-19T07:57:32.259972Z",
     "shell.execute_reply": "2025-01-19T07:57:32.258980Z",
     "shell.execute_reply.started": "2025-01-19T07:57:30.294059Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import metaspore as ms\n",
    "\n",
    "class DemoModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._embedding_size = 16\n",
    "        self._schema_dir = ROOT_DIR + '/schema/'\n",
    "        self._column_name_path = self._schema_dir + 'column_name_demo.txt'\n",
    "        self._combine_schema_path = self._schema_dir + 'combine_schema_demo.txt'\n",
    "        self._sparse = ms.EmbeddingSumConcat(self._embedding_size, self._column_name_path, self._combine_schema_path)\n",
    "        self._sparse.updater = ms.FTRLTensorUpdater()\n",
    "        self._sparse.initializer = ms.NormalTensorInitializer(var=0.01)\n",
    "        self._dense = torch.nn.Sequential(\n",
    "            ms.nn.Normalization(self._sparse.feature_count * self._embedding_size),\n",
    "            torch.nn.Linear(self._sparse.feature_count * self._embedding_size, 1024),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(1024, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._sparse(x)\n",
    "        x = self._dense(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "059b5fa0-8426-4b54-bbbb-df8506fe57fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T07:57:32.262164Z",
     "iopub.status.busy": "2025-01-19T07:57:32.261815Z",
     "iopub.status.idle": "2025-01-19T07:57:32.300947Z",
     "shell.execute_reply": "2025-01-19T07:57:32.300206Z",
     "shell.execute_reply.started": "2025-01-19T07:57:32.262134Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mloaded combine schema from\u001b[m \u001b[32mcombine schema file \u001b[m'/home/dev/workspace/machine-learning/playground/datascience/data/schema/combine_schema_demo.txt'\n",
      "[2025-01-19 15:57:32.263] [info] [local_filesys.cpp:116] Opening local file /home/dev/workspace/machine-learning/playground/datascience/data/schema/combine_schema_demo.txt with mode r\n",
      "integer_feature_1\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(integer_feature_1, StringBKDRHashFunctionOption::name=integer_feature_1)\n",
      "integer_feature_2\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(integer_feature_2, StringBKDRHashFunctionOption::name=integer_feature_2)\n",
      "integer_feature_3\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(integer_feature_3, StringBKDRHashFunctionOption::name=integer_feature_3)\n",
      "integer_feature_4\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(integer_feature_4, StringBKDRHashFunctionOption::name=integer_feature_4)\n",
      "integer_feature_5\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(integer_feature_5, StringBKDRHashFunctionOption::name=integer_feature_5)\n",
      "integer_feature_6\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(integer_feature_6, StringBKDRHashFunctionOption::name=integer_feature_6)\n",
      "integer_feature_7\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(integer_feature_7, StringBKDRHashFunctionOption::name=integer_feature_7)\n",
      "integer_feature_8\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(integer_feature_8, StringBKDRHashFunctionOption::name=integer_feature_8)\n",
      "integer_feature_9\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(integer_feature_9, StringBKDRHashFunctionOption::name=integer_feature_9)\n",
      "integer_feature_10\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(integer_feature_10, StringBKDRHashFunctionOption::name=integer_feature_10)\n",
      "integer_feature_11\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(integer_feature_11, StringBKDRHashFunctionOption::name=integer_feature_11)\n",
      "integer_feature_12\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(integer_feature_12, StringBKDRHashFunctionOption::name=integer_feature_12)\n",
      "integer_feature_13\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(integer_feature_13, StringBKDRHashFunctionOption::name=integer_feature_13)\n",
      "categorical_feature_1\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(categorical_feature_1, StringBKDRHashFunctionOption::name=categorical_feature_1)\n",
      "categorical_feature_2\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(categorical_feature_2, StringBKDRHashFunctionOption::name=categorical_feature_2)\n",
      "categorical_feature_3\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(categorical_feature_3, StringBKDRHashFunctionOption::name=categorical_feature_3)\n",
      "categorical_feature_4\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(categorical_feature_4, StringBKDRHashFunctionOption::name=categorical_feature_4)\n",
      "categorical_feature_5\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(categorical_feature_5, StringBKDRHashFunctionOption::name=categorical_feature_5)\n",
      "categorical_feature_6\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(categorical_feature_6, StringBKDRHashFunctionOption::name=categorical_feature_6)\n",
      "categorical_feature_7\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(categorical_feature_7, StringBKDRHashFunctionOption::name=categorical_feature_7)\n",
      "categorical_feature_8\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(categorical_feature_8, StringBKDRHashFunctionOption::name=categorical_feature_8)\n",
      "categorical_feature_9\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(categorical_feature_9, StringBKDRHashFunctionOption::name=categorical_feature_9)\n",
      "categorical_feature_10\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(categorical_feature_10, StringBKDRHashFunctionOption::name=categorical_feature_10)\n",
      "categorical_feature_11\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(categorical_feature_11, StringBKDRHashFunctionOption::name=categorical_feature_11)\n",
      "categorical_feature_12\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(categorical_feature_12, StringBKDRHashFunctionOption::name=categorical_feature_12)\n",
      "categorical_feature_13\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(categorical_feature_13, StringBKDRHashFunctionOption::name=categorical_feature_13)\n",
      "categorical_feature_14\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(categorical_feature_14, StringBKDRHashFunctionOption::name=categorical_feature_14)\n",
      "categorical_feature_15\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(categorical_feature_15, StringBKDRHashFunctionOption::name=categorical_feature_15)\n",
      "categorical_feature_16\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(categorical_feature_16, StringBKDRHashFunctionOption::name=categorical_feature_16)\n",
      "categorical_feature_17\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(categorical_feature_17, StringBKDRHashFunctionOption::name=categorical_feature_17)\n",
      "categorical_feature_18\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(categorical_feature_18, StringBKDRHashFunctionOption::name=categorical_feature_18)\n",
      "categorical_feature_19\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(categorical_feature_19, StringBKDRHashFunctionOption::name=categorical_feature_19)\n",
      "categorical_feature_20\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(categorical_feature_20, StringBKDRHashFunctionOption::name=categorical_feature_20)\n",
      "categorical_feature_21\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(categorical_feature_21, StringBKDRHashFunctionOption::name=categorical_feature_21)\n",
      "categorical_feature_22\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(categorical_feature_22, StringBKDRHashFunctionOption::name=categorical_feature_22)\n",
      "categorical_feature_23\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(categorical_feature_23, StringBKDRHashFunctionOption::name=categorical_feature_23)\n",
      "categorical_feature_24\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(categorical_feature_24, StringBKDRHashFunctionOption::name=categorical_feature_24)\n",
      "categorical_feature_25\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(categorical_feature_25, StringBKDRHashFunctionOption::name=categorical_feature_25)\n",
      "categorical_feature_26\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash(categorical_feature_26, StringBKDRHashFunctionOption::name=categorical_feature_26)\n",
      "integer_feature_1#categorical_feature_2\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash_combine(bkdr_hash(integer_feature_1, StringBKDRHashFunctionOption::name=integer_feature_1), bkdr_hash(categorical_feature_2, StringBKDRHashFunctionOption::name=categorical_feature_2), BKDRHashCombineFunctionOption)\n",
      "integer_feature_5#categorical_feature_10#categorical_feature_5\n",
      "[2025-01-19 15:57:32.264] [info] add expr bkdr_hash_combine(bkdr_hash(integer_feature_5, StringBKDRHashFunctionOption::name=integer_feature_5), bkdr_hash(categorical_feature_10, StringBKDRHashFunctionOption::name=categorical_feature_10), bkdr_hash(categorical_feature_5, StringBKDRHashFunctionOption::name=categorical_feature_5), BKDRHashCombineFunctionOption)\n"
     ]
    }
   ],
   "source": [
    "module = DemoModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60328c78-d1b5-487c-b0aa-f0e3ceda53da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T07:57:32.302116Z",
     "iopub.status.busy": "2025-01-19T07:57:32.301876Z",
     "iopub.status.idle": "2025-01-19T07:57:32.305737Z",
     "shell.execute_reply": "2025-01-19T07:57:32.305068Z",
     "shell.execute_reply.started": "2025-01-19T07:57:32.302089Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08bd6f3a-df43-4039-bd53-2b514d2a13e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T07:57:33.297936Z",
     "iopub.status.busy": "2025-01-19T07:57:33.297569Z",
     "iopub.status.idle": "2025-01-19T07:57:33.361289Z",
     "shell.execute_reply": "2025-01-19T07:57:33.359614Z",
     "shell.execute_reply.started": "2025-01-19T07:57:33.297898Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds, val_ds = df_train.randomSplit(weights=[0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d0d3c3d-5c5c-452d-a68a-3627cf9077ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T07:57:34.501086Z",
     "iopub.status.busy": "2025-01-19T07:57:34.499851Z",
     "iopub.status.idle": "2025-01-19T07:58:49.983372Z",
     "shell.execute_reply": "2025-01-19T07:58:49.981661Z",
     "shell.execute_reply.started": "2025-01-19T07:57:34.501025Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/19 15:57:34 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=======================================================> (82 + 2) / 84]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36680356 9160261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(train_ds.count(), val_ds.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f9cb73a-c91e-4177-b2eb-80a162407eca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T07:58:49.986841Z",
     "iopub.status.busy": "2025-01-19T07:58:49.986128Z",
     "iopub.status.idle": "2025-01-19T07:58:49.994398Z",
     "shell.execute_reply": "2025-01-19T07:58:49.992751Z",
     "shell.execute_reply.started": "2025-01-19T07:58:49.986783Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_out_path = ROOT_DIR + '/output/dev/model_out/'\n",
    "estimator = ms.PyTorchEstimator(module=module,\n",
    "                                worker_count=1,\n",
    "                                server_count=1,\n",
    "                                model_out_path=model_out_path,\n",
    "                                experiment_name='0.1',\n",
    "                                input_label_column_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08920dae-b03e-4dab-a9aa-8fe4fb0e0ebe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T07:58:49.996744Z",
     "iopub.status.busy": "2025-01-19T07:58:49.996075Z",
     "iopub.status.idle": "2025-01-19T07:58:50.004770Z",
     "shell.execute_reply": "2025-01-19T07:58:50.003534Z",
     "shell.execute_reply.started": "2025-01-19T07:58:49.996688Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['label', 'integer_feature_1', 'integer_feature_2', 'integer_feature_3', 'integer_feature_4', 'integer_feature_5', 'integer_feature_6', 'integer_feature_7', 'integer_feature_8', 'integer_feature_9', 'integer_feature_10', 'integer_feature_11', 'integer_feature_12', 'integer_feature_13', 'categorical_feature_1', 'categorical_feature_2', 'categorical_feature_3', 'categorical_feature_4', 'categorical_feature_5', 'categorical_feature_6', 'categorical_feature_7', 'categorical_feature_8', 'categorical_feature_9', 'categorical_feature_10', 'categorical_feature_11', 'categorical_feature_12', 'categorical_feature_13', 'categorical_feature_14', 'categorical_feature_15', 'categorical_feature_16', 'categorical_feature_17', 'categorical_feature_18', 'categorical_feature_19', 'categorical_feature_20', 'categorical_feature_21', 'categorical_feature_22', 'categorical_feature_23', 'categorical_feature_24', 'categorical_feature_25', 'categorical_feature_26']\n"
     ]
    }
   ],
   "source": [
    "column_names = []\n",
    "with open(f'{ROOT_DIR}/schema/column_name_demo.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        column_names.append(line.split(' ')[1].strip())\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9de4019b-169b-4ace-9b53-ded7f75504fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T07:58:58.499958Z",
     "iopub.status.busy": "2025-01-19T07:58:58.499236Z",
     "iopub.status.idle": "2025-01-19T07:58:58.840957Z",
     "shell.execute_reply": "2025-01-19T07:58:58.838899Z",
     "shell.execute_reply.started": "2025-01-19T07:58:58.499889Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-19 15:58:58.512] [info] PS job with coordinator address 172.50.0.4:38171 started.\n",
      "[2025-01-19 15:58:58.513] [info] PSRunner::RunPS: pid: 7552, tid: 8021, thread: 0x7f2a0062c700\n",
      "[2025-01-19 15:58:58.513] [info] PSRunner::RunPSCoordinator: pid: 7552, tid: 8021, thread: 0x7f2a0062c700\n",
      "[2025-01-19 15:58:58.515] [info] ActorProcess::Receiving: Coordinator pid: 7552, tid: 8026, thread: 0x7f29f87e8700\n",
      "25/01/19 15:58:58 WARN TaskSetManager: Lost task 0.0 in stage 14.0 (TID 260) (172.50.0.18 executor 1): java.io.IOException: Cannot run program \"/usr/bin/python3\": error=2, No such file or directory\n",
      "\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)\n",
      "\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:216)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:134)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:157)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.io.IOException: error=2, No such file or directory\n",
      "\tat java.base/java.lang.ProcessImpl.forkAndExec(Native Method)\n",
      "\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:314)\n",
      "\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:244)\n",
      "\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)\n",
      "\t... 17 more\n",
      "\n",
      "25/01/19 15:58:58 WARN TaskSetManager: Lost task 0.0 in stage 15.0 (TID 261) (172.50.0.19 executor 0): java.io.IOException: Cannot run program \"/usr/bin/python3\": error=2, No such file or directory\n",
      "\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)\n",
      "\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:216)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:134)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:157)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.io.IOException: error=2, No such file or directory\n",
      "\tat java.base/java.lang.ProcessImpl.forkAndExec(Native Method)\n",
      "\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:314)\n",
      "\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:244)\n",
      "\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)\n",
      "\t... 17 more\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Could not recover from a failed barrier ResultStage. Most recent failure reason: Stage failed because barrier task ResultTask(14, 0) finished unsuccessfully.\njava.io.IOException: Cannot run program \"/usr/bin/python3\": error=2, No such file or directory\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:216)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:134)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:157)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.base/java.lang.ProcessImpl.forkAndExec(Native Method)\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:314)\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:244)\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)\n\t... 17 more\n\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:2111)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2857)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/metaspore/estimator.py:965\u001b[0m, in \u001b[0;36mPyTorchEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_output()\n\u001b[1;32m    964\u001b[0m launcher \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_launcher(dataset, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 965\u001b[0m \u001b[43mlauncher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m module \u001b[38;5;241m=\u001b[39m launcher\u001b[38;5;241m.\u001b[39magent_object\u001b[38;5;241m.\u001b[39mmodule\n\u001b[1;32m    967\u001b[0m module\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/metaspore/estimator.py:608\u001b[0m, in \u001b[0;36mPyTorchLauncher.launch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agent_attributes\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_agent_attributes)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keep_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 608\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/metaspore/ps_launcher.py:144\u001b[0m, in \u001b[0;36mPSLauncher.launch_agent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m     args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserver_count\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server_count\n\u001b[1;32m    143\u001b[0m     args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124magent_attributes\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agent_attributes\n\u001b[0;32m--> 144\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspark_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keep_session:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/asyncio/futures.py:178\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/usr/lib/python3.8/asyncio/tasks.py:282\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    280\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 282\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_must_cancel:\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/metaspore/agent.py:326\u001b[0m, in \u001b[0;36mAgent._launch\u001b[0;34m(cls, args, spark_session, launcher)\u001b[0m\n\u001b[1;32m    324\u001b[0m futures\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_launch_workers(args, spark_session))\n\u001b[1;32m    325\u001b[0m futures\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_launch_coordinator(args, spark_session, launcher))\n\u001b[0;32m--> 326\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mfutures)\n",
      "File \u001b[0;32m/usr/lib/python3.8/asyncio/tasks.py:349\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 349\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/metaspore/agent.py:291\u001b[0m, in \u001b[0;36mAgent._launch_servers.<locals>.launch_servers\u001b[0;34m()\u001b[0m\n\u001b[1;32m    289\u001b[0m     spark_context \u001b[38;5;241m=\u001b[39m spark_session\u001b[38;5;241m.\u001b[39msparkContext\n\u001b[1;32m    290\u001b[0m     rdd \u001b[38;5;241m=\u001b[39m spark_context\u001b[38;5;241m.\u001b[39mparallelize(\u001b[38;5;28mrange\u001b[39m(server_count), server_count)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbarrier\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     loop\u001b[38;5;241m.\u001b[39mcall_soon_threadsafe(future\u001b[38;5;241m.\u001b[39mset_result, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/rdd.py:1197\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[1;32m   1196\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1197\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Could not recover from a failed barrier ResultStage. Most recent failure reason: Stage failed because barrier task ResultTask(14, 0) finished unsuccessfully.\njava.io.IOException: Cannot run program \"/usr/bin/python3\": error=2, No such file or directory\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:216)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:134)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:157)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.io.IOException: error=2, No such file or directory\n\tat java.base/java.lang.ProcessImpl.forkAndExec(Native Method)\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:314)\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:244)\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)\n\t... 17 more\n\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:2111)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2857)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "model = estimator.fit(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5c78fbe-1a8f-444e-979f-6634ee61d232",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T07:50:05.092938Z",
     "iopub.status.busy": "2025-01-19T07:50:05.092335Z",
     "iopub.status.idle": "2025-01-19T07:50:05.435328Z",
     "shell.execute_reply": "2025-01-19T07:50:05.433787Z",
     "shell.execute_reply.started": "2025-01-19T07:50:05.092879Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(val_ds)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "result = model.transform(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4019a0b9-af5a-4d43-af17-37cd3f4f5e8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T07:50:20.125958Z",
     "iopub.status.busy": "2025-01-19T07:50:20.125199Z",
     "iopub.status.idle": "2025-01-19T07:50:20.161359Z",
     "shell.execute_reply": "2025-01-19T07:50:20.159567Z",
     "shell.execute_reply.started": "2025-01-19T07:50:20.125894Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresult\u001b[49m\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6611b73-c5a8-46aa-be10-146f20d482ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T07:51:34.288265Z",
     "iopub.status.busy": "2025-01-19T07:51:34.287589Z",
     "iopub.status.idle": "2025-01-19T07:51:34.579177Z",
     "shell.execute_reply": "2025-01-19T07:51:34.578039Z",
     "shell.execute_reply.started": "2025-01-19T07:51:34.288196Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluation\n\u001b[0;32m----> 2\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m \u001b[43mevaluation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBinaryClassificationEvaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m test_auc \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mevaluate(result)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_auc: \u001b[39m\u001b[38;5;132;01m%g\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m test_auc)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/__init__.py:135\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/evaluation.py:239\u001b[0m, in \u001b[0;36mBinaryClassificationEvaluator.__init__\u001b[0;34m(self, rawPredictionCol, labelCol, metricName, weightCol, numBins)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;124;03m__init__(self, \\\\*, rawPredictionCol=\"rawPrediction\", labelCol=\"label\", \\\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;124;03m         metricName=\"areaUnderROC\", weightCol=None, numBins=1000)\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28msuper\u001b[39m(BinaryClassificationEvaluator, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_java_obj\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.spark.ml.evaluation.BinaryClassificationEvaluator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muid\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(metricName\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mareaUnderROC\u001b[39m\u001b[38;5;124m\"\u001b[39m, numBins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m    243\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/wrapper.py:80\u001b[0m, in \u001b[0;36mJavaWrapper._new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03mReturns a new Java object.\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     79\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n\u001b[0;32m---> 80\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     82\u001b[0m java_obj \u001b[38;5;241m=\u001b[39m _jvm()\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m java_class\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.ml import evaluation\n",
    "evaluator = evaluation.BinaryClassificationEvaluator()\n",
    "test_auc = evaluator.evaluate(result)\n",
    "print('test_auc: %g' % test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130b6ddc-b6a9-4506-9b67-db0683c6d965",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
