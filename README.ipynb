{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "500ab5de-f464-4acf-b81e-e6973fcb59af",
   "metadata": {},
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4a51dfc-3a62-41b0-8d15-a17e20523044",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe7003e2-bde3-4629-9966-7437eca00a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96669c3d-65a2-4c0e-836e-657b06745e8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 安装 pytorch (gpu)\n",
    "#!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "829fcab5-9713-4621-9200-b77c25c405e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T15:39:46.168999Z",
     "start_time": "2024-07-01T15:39:05.781604Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 安装依赖包\n",
    "#!pip3 install numpy pandas seaborn matplotlib scipy scikit-learn tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197c0e64-f177-42e3-a037-f0cba993dc2c",
   "metadata": {},
   "source": [
    "# 初始化 PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d230e77-b6c1-43d6-a49f-6b9aa2a36d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, greatest, lit, abs\n",
    "from configparser import ConfigParser\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "\n",
    "# 设置 JDK\n",
    "os.environ[\"JAVA_HOME\"] = \"/lib/jvm/java-17-openjdk-amd64\"\n",
    "\n",
    "print(pyspark.__version__)\n",
    "\n",
    "# 添加 jar 包\n",
    "jars_dir = \"/home/jovyan/jars\"\n",
    "jars_list = [\n",
    "    os.path.join(jars_dir, f) for f in os.listdir(jars_dir) if f.endswith(\".jar\")\n",
    "]\n",
    "jars_str = \",\".join(jars_list)\n",
    "print(jars_str)\n",
    "\n",
    "# 创建本地 SparkSession（local 模式）\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"LocalPySparkExample\")\n",
    "    .config(\"spark.jars\", jars_str)\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"10g\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"100\")\n",
    "    .config(\"spark.default.parallelism\", \"100\")\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print('Spark Version: ', spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06c89d7-2ab5-4fff-8439-a1c20fa5a04a",
   "metadata": {},
   "source": [
    "# Jar 包链接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69d87f5d-4971-4784-8217-77f28486df7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-08-19 03:16:36--  https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.4.1/hadoop-aws-3.4.1.jar\n",
      "Resolving repo1.maven.org (repo1.maven.org)... 199.232.196.209, 199.232.192.209\n",
      "Connecting to repo1.maven.org (repo1.maven.org)|199.232.196.209|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 865554 (845K) [application/java-archive]\n",
      "Saving to: ‘/home/jovyan/jars/hadoop-aws-3.4.1.jar’\n",
      "\n",
      "hadoop-aws-3.4.1.ja 100%[===================>] 845.27K  1.06MB/s    in 0.8s    \n",
      "\n",
      "2025-08-19 03:16:37 (1.06 MB/s) - ‘/home/jovyan/jars/hadoop-aws-3.4.1.jar’ saved [865554/865554]\n",
      "\n",
      "--2025-08-19 03:16:38--  https://repo1.maven.org/maven2/software/amazon/awssdk/bundle/2.32.24/bundle-2.32.24.jar\n",
      "Resolving repo1.maven.org (repo1.maven.org)... 146.75.92.209\n",
      "Connecting to repo1.maven.org (repo1.maven.org)|146.75.92.209|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 684297060 (653M) [application/java-archive]\n",
      "Saving to: ‘/home/jovyan/jars/bundle-2.32.24.jar’\n",
      "\n",
      "bundle-2.32.24.jar  100%[===================>] 652.60M   903KB/s    in 7m 10s  \n",
      "\n",
      "2025-08-19 03:23:49 (1.52 MB/s) - ‘/home/jovyan/jars/bundle-2.32.24.jar’ saved [684297060/684297060]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aliyun OSS\n",
    "#!wget -P /home/jovyan/jars https://repo1.maven.org/maven2/com/aliyun/oss/aliyun-sdk-oss/3.18.2/aliyun-sdk-oss-3.18.2.jar\n",
    "#!wget -P /home/jovyan/jars https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aliyun/3.4.1/hadoop-aliyun-3.4.1.jar\n",
    "# Paimon\n",
    "#!wget -P /home/jovyan/jars https://repository.apache.org/content/groups/snapshots/org/apache/paimon/paimon-spark-3.4/1.3-SNAPSHOT/paimon-spark-3.4-1.3-20250707.003407-18.jar\n",
    "#!wget -P /home/jovyan/jars https://repo1.maven.org/maven2/org/apache/paimon/paimon-oss/1.2.0/paimon-oss-1.2.0.jar\n",
    "# AWS (包含 s3a 支持)\n",
    "!wget -P /home/jovyan/jars https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.4.1/hadoop-aws-3.4.1.jar\n",
    "!wget -P /home/jovyan/jars https://repo1.maven.org/maven2/software/amazon/awssdk/bundle/2.32.24/bundle-2.32.24.jar\n",
    "# 其他\n",
    "#!wget -P /home/jovyan/jars https://repo1.maven.org/maven2/org/apache/commons/commons-configuration2/2.12.0/commons-configuration2-2.12.0.jar\n",
    "#!wget -P /home/jovyan/jars https://repo1.maven.org/maven2/org/jdom/jdom2/2.0.6.1/jdom2-2.0.6.1.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3358ff-0954-4a25-a346-ea352e2fa118",
   "metadata": {},
   "source": [
    "# 访问 OSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed4c8e6-3bb2-4bb3-8a79-8e2d503b4a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取 OSS 配置\n",
    "config_path = os.path.expanduser(\"~/.ossutilconfig\")\n",
    "parser = ConfigParser()\n",
    "parser.read(config_path)\n",
    "endpoint = parser.get(\"Credentials\", \"endpoint\")\n",
    "access_key_id = parser.get(\"Credentials\", \"accessKeyID\")\n",
    "access_key_secret = parser.get(\"Credentials\", \"accessKeySecret\")\n",
    "\n",
    "# 创建本地 SparkSession（local 模式）\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"LocalPySparkExample\")\n",
    "    .config(\"spark.jars\", jars_str)\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"10g\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"100\")\n",
    "    .config(\"spark.default.parallelism\", \"100\")\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35\")\n",
    "    .config(\n",
    "        \"spark.hadoop.fs.oss.impl\",\n",
    "        \"org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem\",\n",
    "    )\n",
    "    .config(\"spark.hadoop.fs.oss.accessKeyId\", access_key_id)\n",
    "    .config(\"spark.hadoop.fs.oss.accessKeySecret\", access_key_secret)\n",
    "    .config(\"spark.hadoop.fs.oss.endpoint\", endpoint)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4258e62d-2271-4efd-9f88-2f4e69f721e7",
   "metadata": {},
   "source": [
    "# 访问 MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ef2609-1fa8-4d07-98c1-97be698ea8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 读取 MinIO 配置\n",
    "config_path = os.path.expanduser(\"~/.minioconfig\")\n",
    "parser = ConfigParser()\n",
    "parser.read(config_path)\n",
    "endpoint = parser.get(\"Credentials\", \"endpoint\")\n",
    "access_key_id = parser.get(\"Credentials\", \"accessKeyID\")\n",
    "access_key_secret = parser.get(\"Credentials\", \"accessKeySecret\")\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"PySpark MinIO Example\")\n",
    "    # S3A 配置\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", endpoint)                   # MinIO 服务地址\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", access_key_id)            # MinIO 用户名\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", access_key_secret)        # MinIO 密码\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")            # 必须设为 true，MinIO 用路径模式\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# 读取数据\n",
    "df = spark.read.csv(\"s3a://mybucket/data.csv\", header=True, inferSchema=True)\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
